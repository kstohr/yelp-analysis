---
title: "Yelp Challenge Data Set - Exploratory Analysis"
author: "Kate Stohr"
date: "November 2, 2015"
output: html_document
---

```{r}
options(digits=4, scipen=999)
setwd("~/Documents/Coursera/Capstone/yelp-analysis")
```

# Let's play with the business data

```{r}
library(jsonlite)
library(dplyr)
library(ggmap)
library(leaflet)
library(maps)
library(scales)
dir.data <-file.path('.', 'data', 'yelp_dataset_challenge_academic_dataset')
```

##Load the data 

```{r cache=TRUE, echo=FALSE}
bus.fn<-'yelp_academic_dataset_business.json'
bus.df <- stream_in(file(file.path(dir.data, bus.fn)))
bus <- flatten(bus.df)
rm(bus.df)
bus$cat <- sapply(bus$categories, toString)
```

```{r cache=TRUE, echo=FALSE}
checkin.fn<-'yelp_academic_dataset_checkin.json'
checkin.df <- stream_in(file(file.path(dir.data, checkin.fn)))
checkin <- flatten(checkin.df)
rm(checkin.df)

```

```{r eval=FALSE, echo=FALSE, cache=TRUE}

tip.fn<-'yelp_academic_dataset_tip.json'
tip.df <- stream_in(file(file.path(dir.data, tip.fn)))
tip<-tip.df
#no need to flatten 
```

```{r eval=FALSE, echo=FALSE, cache=TRUE}
##NOT NEEDED 
user.fn<-'yelp_academic_dataset_user.json'
user.df <- stream_in(file(file.path(dir.data, user.fn)))
user <- flatten(user.df)
rm(user.df)
```

```{r eval=FALSE, echo=FALSE, cache=TRUE}
##NOT NEEDED 
review.fn<-'yelp_academic_dataset_review.json'
review.df <- stream_in(file(file.path(dir.data, review.fn)))
review.df.flat <- flatten(review.df)
rm(review.df)
review<-review.df.flat
```

Save files to RDS for future access. 
```{r eval=FALSE, echo=FALSE, cache=TRUE}
saveRDS(bus.df.flat, file.path(dir.data, "bus.rds"))
saveRDS(checkin.df.flat, file.path(dir.data, "checkin.rds"))
saveRDS(review.df.flat, file.path(dir.data, "review.rds"))
```


## Exploratory plotting
Build a basic map of the business data. 
```{r}
map("world", ylim=c(10,70), xlim=c(-130,25), col="gray60")
points(bus$longitude, bus$latitude,pch=19, col="cyan4")
```

Identify metro areas by using kmeans clustering to isolate nearby lat/long coordinates. 

```{r, echo=FALSE}
cities<-c('Edinburgh, UK', 'Karlsruhe, Germany', 'Montreal, Canada', 'Waterloo, Canada', 'Pittsburgh, PA', 'Charlotte, NC', 'Urbana-Champaign, IL', 'Phoenix, AZ', 'Las Vegas, NV', 'Madison, WI')
city.centres<-geocode(cities)
set.seed(222)
geo.cluster<-kmeans(bus[,c('longitude','latitude')],city.centres) #use kmeans to identify metro areas
```

Subset by metro area. Explore data for Philadelphia. 

```{r, echo=FALSE}
bus_by_cluster <- as.data.frame(cbind(bus, geo.cluster$cluster)) #Merge cluster assignment back to business data
buscl_5<-subset(bus, geo.cluster$cluster==5) #Pull data for Philadelphia
m5 <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addMarkers(lng=buscl_5$longitude, lat=buscl_5$latitude, popup=buscl_5$name)
print(m5)
```

Checkout the checkin data set 

```{r}
checkin$total<-rowSums(checkin[,3:170], na.rm = TRUE)
summary(checkin$total) #ignore NA's

checkin$total<-rowSums(checkin[,3:170], na.rm = FALSE)
summary(checkin$total)#include NA's

```



Take a look for Philly businesses in the check-in data set. 

```{r}
checkinpa<-buscl_5$business_id %in% checkin$business_id  #isolate the Philly bus id's in checkins. 

checkcl_5<-checkin[checkinpa,] #subset to include matching ids. 
percent_checkins_pa<-length(checkcl_5)/length(buscl_5)
percent(percent_checkins_pa)
checkcl_5$total<-rowSums(checkcl_5[,3:170], na.rm = FALSE)
checkcl_5<-cbind(business_id=checkcl_5$business_id, total_checkins=checkcl_5$total)
checkinpa<-merge(buscl_5, checkcl_5, by = "business_id")
checkinpa$total_checkins<-as.numeric(checkinpa$total_checkins)
quants<-quantile(checkinpa$total_checkins)
quants
```

Map checkin points to see if they are grouped in anyway. 

```{r}
m5_checkins <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addCircleMarkers(lng=checkinpa$longitude, lat=checkinpa$latitude, popup=checkinpa$name, color = c('red'), stroke = FALSE, fillOpacity = 0.5, radius = 2, group = "Check-ins")
print(m5_checkins)
```

High frequency checkins align with traffic arteries.

Overlay checkins on all PA businesses. 

```{r}
m5_checkins <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addCircleMarkers(lng=buscl_5$longitude, lat=buscl_5$latitude, popup=buscl_5$name, color = c('blue'), stroke = FALSE, fillOpacity = 0.5, radius = 2, group = "All Businesses") %>%
  addCircleMarkers(lng=checkinpa$longitude, lat=checkinpa$latitude, popup=checkinpa$name, color = c('red'), stroke = FALSE, fillOpacity = 0.5, radius = 2, group = "Check-ins")
print(m5_checkins)
```

Interesting. Some businesses have checkin's and some do not. What are the factors that correlate highly with checkins? What factors have a low correlation with checkins? How can business owners benchmark the number of checkins they should expect conditioned on location and business category? Or, conditioned on businesses that are most like them? 

Steps: 
1) Build a data file that aggregates checkins for each businesses and includes businesses that have no checkins. 
1. Check on data time period. 
Unfortunately there are no datetime stamps included in the checkin data. Check other data sets to see what timeframe they represent. 

```{r} 
## find out what period of time the review data was collected in. 
review$date<-ymd(review$date)
min(review$date)
max(review$date)
start<-min(review$date)
end<-max(review$date)
end - start
(end-start)/365
start.yelp<-(ymd("2010-01-15"))
(end-start.yelp)/365
```


```{r} 
## find out what period of time the tip data was collected in. 
tip$date<-ymd(tip$date)
min(tip$date)
max(tip$date)
start<-min(tip$date)
end<-max(tip$date)
end - start
(end-start)/365

```


The reviews were collected between the period of Oct. 10, 2004 and Jan. 8, 2015 (10 years and 3 months). Tip data starts in April 15, 2009 and ends on Jan 22, 2015. This roughly corresponds to the period that yelp made its mobile apps available (2008 onwards).  However, Yelp didn't introduce it's checkin feature on the iphone until Jan 15, 2010. So the time period covered can only be just shy of 5 years (4.984 years). 

Therefore, for the purposes of this analysis, we're going to assume that the checkin data is "cumulative" and assume the data covers a 5-year period. Good for interpreting active times, but not for predicting business info, because you can't convert the data into a rate for the specific business as you don't have data on how long businesses have been on Yelp. And the length of time a business has been on yelp is very likely to impact the number of reviews, checkins and tips it has recieved cumulatively.  

Ok, so using the time data, what can we do? Well we can compare the time data to the location of the checkin by cross referencing the business location. Start by mapping just one time range. 

```{r}

m5_checkins <- leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addCircleMarkers(lng=buscl_5$longitude, lat=buscl_5$latitude, popup=buscl_5$name, color = c('blue'), stroke = FALSE, fillOpacity = 0.5, radius = 2, group = "All Businesses") %>%
  addCircleMarkers(lng=checkinpa$longitude, lat=checkinpa$latitude, popup=checkinpa$name, color = c('red'), stroke = FALSE, fillOpacity = 0.5, radius = 2, group = "Check-ins")
print(m5_checkins)

```



QUESTION: 
Can I predict what area of the city will get the most check-in's at any given time? Then create a time-series map that allows users to find what will most likely be the most active or least active places in a city at a given time? 

This could be used to allow folks to plan canvasing routes, such as for sales outreach, advocacy campaigns or political campaigns. Or, to identify the best locations for an event. Or, for planning departments to evaluate where to locate public events for maximum impact (to bring people to a place that could be more active) or exposure(hold an event at a time and place where it is most likely to get maximum exposure or least likely to impact traffice, etc.)

Added bonus: Trace a map through the city for maximum exposure in any given day. 

STEPS: 

1) Build the write data frame 
 - summarize checkin data by location. 
     
2) Map the place/s that has the most checkin's at a given time.   


     



2) Include other factors that are available in the bus file. Look at other files to see if there may be factors that are of interest. 
3) Build a prediction model that finds like businesses (regression? How to test?)

4) Understand time series data for checkins. Is it seasonal? 

5) Build code that averages check in data for like businessses and generates a prediction of average checkins/time period 


NOTES:
rgeos::gDistance dist parameter for a cutoff distance. 
Would be good to add a layer for commercial zoning. Would be interesting to condition checkins on weather /seasonality. 



